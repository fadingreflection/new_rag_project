{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8a6c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, float16\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "DEVICE = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Current device is: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b89e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем модель …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e54ca57f6e74488b9050ed4b21f3d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anastasiya Fedotova\\Desktop\\DS&ML\\github\\rag_testing\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Anastasiya Fedotova\\.cache\\huggingface\\hub\\models--unsloth--Qwen2.5-3B-unsloth-bnb-4bit. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c356ad7ecef4230ab59f979ccd4e512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340bf07bb80e4c748ef5a89850c9c22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bcfcd83b5342d9ad489f5ccf5320ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03065165b2ca402dacd07d6a283c4934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c5e2fab20b421f846d5a4159f40b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a2b304eaa84c80ab7a55c1509e59e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec30974c8344d23b563d19c2517f092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5f75eb413e44349b2559ae00aaf733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# MODEL_ID = \"deepseek-ai/deepseek-coder-7b-instruct\"\n",
    "MODEL_ID = \"unsloth/Qwen2.5-3B-unsloth-bnb-4bit\"\n",
    "\n",
    "# Квантуем в 4 бита, чтобы поместилось в VRAM 6–8 ГБ\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                          # включить 4-битное квантование\n",
    "    bnb_4bit_quant_type=\"nf4\",                  # тип квантования: \"nf4\" (Normalized Float 4) или \"fp4\"\n",
    "    bnb_4bit_use_double_quant=True,             # включить двойное квантование (дополнительная компрессия)\n",
    "    bnb_4bit_compute_dtype=float16        # тип данных для вычислений (например, bfloat16 (недоступен на T4), float16)\n",
    ")\n",
    "\n",
    "print(\"Загружаем модель …\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ff9ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объясни, пожалуйста, что такое ‘State of the Union’. Ответь одним абзацем на русском. В английском языке это выражение означает «Социальный статус», «Состояние здоровья», «Состояние общества». Т.е. оно может быть использовано в широком смысле: об общем «уровне» общественного развития, социальной жизни, политического развития страны. Также оно может употребляться в более конкретном и специфическом смысле, чтобы охарактеризовать конкретный «уровень» (степень) развития какой-либо «сферы» общественной жизни, например, экономики или медицины. В данном случае «State of the Union» употребляется в более широком смысле и говорит о том, что президент США дает доклад об общем «уровне» экономического и социального развития страны.\n"
     ]
    }
   ],
   "source": [
    "def ask_DeepSeek(prompt: str):\n",
    "    resp = llm_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=200,     # генерируем ≤ 200 новых токенов\n",
    "        do_sample=True,         # сэмплирование позволяет модели додумывать\n",
    "        truncation=True,        # обрываем слишком длинные ответы\n",
    "        top_k=20,               # топ-20 наиболее вероятных токенов на выходе генерации\n",
    "        num_return_sequences=1, # 1 вариант ответа\n",
    "        eos_token_id=tokenizer.eos_token_id, # что считать концом последовательности\n",
    "    )[0][\"generated_text\"]\n",
    "    print(resp)\n",
    "\n",
    "ask_DeepSeek(\"Объясни, пожалуйста, что такое ‘State of the Union’. Ответь одним абзацем на русском.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bf3758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Документов: 6284\n",
      "Ева  — в авраамических религиях — праматерь всех людей, первая женщина, жена Адама, созданная из его ребра, мать Каина, Авеля и Сифа.Библейский рассказ о сотворении Адама и Евы, грехопадении и изгнани …\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "wiki_ds = load_dataset(\"Den4ikAI/russian_cleared_wikipedia\", split=\"train\")\n",
    "\n",
    "# Document\n",
    "corpus_docs = [\n",
    "    Document(page_content=rec[\"sample\"])\n",
    "    for rec in wiki_ds\n",
    "]\n",
    "\n",
    "print(\"Документов:\", len(corpus_docs))\n",
    "print(corpus_docs[0].page_content[:200], \"…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9105ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import TextLoader  # загружает текстовые файлы и превращает их в объекты Document для LangChain.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # рекурсивно разбивает длинный текст на более мелкие фрагменты (chunks).\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # оборачивает модели из HuggingFace для получения эмбеддингов текста.\n",
    "from langchain.vectorstores import Chroma  # векторное хранилище Chroma: сохраняет и ищет эмбеддинги.\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline  # использует HuggingFace Transformers pipeline как LLM-модуль в LangChain.\n",
    "from langchain.chains import RetrievalQA  # готовая цепочка «поиск + генерация ответа» (Retrieval-augmented QA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73e996d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чанков: 67536\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512, chunk_overlap=50\n",
    ")\n",
    "\n",
    "docs = splitter.split_documents(corpus_docs)\n",
    "print(\"Чанков:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a2817a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fc8c44d3724f23b3ae1005dd848f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anastasiya Fedotova\\Desktop\\DS&ML\\github\\rag_testing\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Anastasiya Fedotova\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05057acf8d1644a98726e2ee3b6e10b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a6e9357f314048bc003fb02ab13292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3c49504db34907a2a993530ef83b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f70878ac07047f6b88ca9d86aa1da9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4849e413a9784952b52e97ae53b91168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d12fc768524a8aa0f7f19dc46b1702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6862f6cb04409b93edbf8d21878b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23d84e33e474eacb3eec25b14eb0d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e40dca185b46af9fe1a3ca873ed410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "C:\\Users\\Anastasiya Fedotova\\AppData\\Local\\Temp\\ipykernel_26980\\3103602615.py:12: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist() # в нашем рабочем пространстве создалась директория - векторное хранилище\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    # model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_name=\"intfloat/multilingual-e5-base\", #Russian semantic search\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,               # либо corpus_docs, если без сплиттера\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"chroma_ragmini\"  # директория для хранения векторной базы\n",
    ")\n",
    "vectordb.persist() # в нашем рабочем пространстве создалась директория - векторное хранилище"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "818e667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_cfg = GenerationConfig.from_pretrained(MODEL_ID)\n",
    "gen_cfg.max_length     = 2048     # допустим любой контекст до 2048 токенов\n",
    "gen_cfg.max_new_tokens = 128      # ограничиваем длину генерируемого ответа\n",
    "# gen_cfg.do_sample = True\n",
    "# gen_cfg.temperature = 0.0\n",
    "# gen_cfg.top_p = 0.9\n",
    "\n",
    "model.generation_config = gen_cfg   # привязываем к модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff8bda59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "simple_custom_prompt = PromptTemplate(\n",
    "    template=\"\"\"<think>\n",
    "Use the following context to answer the question. Be precise and short. Avoid repetitions. Do not explain the answer unless asked to do so.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "First think logically, then give me the answer.\n",
    "</think>\n",
    "<answer>\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "simple_custom_prompt = PromptTemplate(\n",
    "    template=\"\"\"<think>\n",
    "Сначала подумай, потом ответь. Избегай повторений и будь краток, если не попросят пояснений.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "</think>\n",
    "<answer>\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=llm_pipeline,          # тот, что мы собрали для DeepSeek\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 128,      # генерируем 128 новых токенов\n",
    "        \"temperature\": 0.1,         # опционально\n",
    "        \"do_sample\": False          # чтобы ответ был детерминирован\n",
    "    }\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": simple_custom_prompt},  # ваш кастомный промпт\n",
    "    return_source_documents=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af137e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: <answer>Сказка о рыбке и рыбаке рассказывает следующую историю: Однажды рыбак ловил рыбу на реке, и рыба попала в его ловушку. Рыбка была очень красивая и умная. Она знала, что рыбак собирался уйти, и решила развлечь его и остановить. Рыбка начала петь и плясать для рыбака, и рыбак посмеялся над ней и решил заставить ее работать. Он сказал рыбке, что если она не будет ловиться, он ее отпустит. Рыбка согласилась на это и стала упорно пытаться ловить рыбу. В результате рыбак пришлось отпустить рыбку, и она ушла в воду. Рыбка пошла на берег и сказала рыбаку, что он должен было ей быть благодарен, потому что она помогла ему почувствовать свою силу и упорство. Сказка о рыбке и рыбаке учит нас, что иногда нужно быть усердным и не бросаться в переделки, а если необходимо, то можно быть умным и убедить другого человека, а не заставить того работать, чтобы добраться до него.\n"
     ]
    }
   ],
   "source": [
    "question = \"Расскажи сказку про рыбака и рыбку\"\n",
    "\n",
    "answer_str = qa_chain.run(question)\n",
    "marker = \"</think>\"\n",
    "print(\"Ответ:\", answer_str.split(marker)[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b59d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Did Lincoln sign the National Banking Act of 1863? First say yes or no, then justify shortly.\"\n",
    "\n",
    "# answer_str = qa_chain.run(question)\n",
    "# print(\"Ответ:\",  answer_str.split(marker)[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d7857ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: <answer>Яблоки антоновского сорта и белый налив - это два разных сорта яблок. Яблоки антоновского сорта имеют более крупные размеры и ярко-красный цвет с белыми точками на кожуре. Они богаты витамином С и содержат низкую калорийность, что делает их идеальным выбором для тех, кто следит за своим весом или диетой. Белый налив, напротив, имеет более маленькие размеры и имеет более мягкий мякоть. Он также богат витамином С и содержит низкую калорийность, но его кислотность более насыщена, чем у антоновского сорта.</answer>\n",
      "\n",
      "Source documents:\n",
      "в тесте, приготовляют начинки для пирогов, тортов и пирожных, очень популярны яб …\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 1}), # здесь мы подаем ограничение на кол-во возвращаемых док-ов\n",
    "    chain_type_kwargs={\"prompt\": simple_custom_prompt},\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# попробуем другую функцию - она возвращает и использованные документы\n",
    "response = qa_chain.invoke(\"Коротко расскажи про разницу яблок антоновка и яблок сорта белый налив\")\n",
    "print(\"Ответ:\", response[\"result\"].split(marker, 1)[1].strip())\n",
    "print(\"\\nSource documents:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(doc.page_content[:80], \"…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cc7fc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='в тесте, приготовляют начинки для пирогов, тортов и пирожных, очень популярны яблочные пироги.Сушёные яблоки являются хорошим источником легкоусваиваемых сахаров, микроэлементов, а в семенах одного среднего плода содержится о о суточной нормы йода.Например, яблоки антоновского сорта в 100 граммах при калорийности в 48 ккал содержат: 0,3 г белков, 11,5 г углеводов, 0,02\\xa0мг витамина B1, 4,9\\xa0мг витамина С, 16 мг кальция и 86 мг калия.Виды:Ещё более 200 видовых названий этого рода имеют в The Plant List статус')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed2fa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json, tqdm\n",
    "\n",
    "# # функция для удаления всего до и включая маркер </think>\n",
    "# def strip_cot(raw: str, marker: str = \"</think>\") -> str:\n",
    "#     parts = raw.split(marker, 1)\n",
    "#     # если не влез COT в токены, выводим ответ как есть\n",
    "#     return parts[1].strip() if len(parts) == 2 else raw.strip()\n",
    "\n",
    "# testset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\")[\"test\"]\n",
    "\n",
    "# total, correct = 0, 0\n",
    "\n",
    "# print(\"Проверка 10 примеров:\\n\")\n",
    "# for sample in tqdm.tqdm(testset.select(range(4, 14))):\n",
    "#     q, gold = sample[\"question\"], sample[\"answer\"]\n",
    "\n",
    "#     result_dict = qa_chain.invoke(q)\n",
    "#     raw = result_dict[\"result\"]\n",
    "#     # обрезаем COT\n",
    "#     pred = strip_cot(raw)\n",
    "\n",
    "#     print(f\"\\nВопрос: {q}\")\n",
    "#     print(f\"Ожидаемые ответы: {gold}\")\n",
    "#     print(f\"Ответ модели: {pred}\")\n",
    "\n",
    "#     if any(g.lower() in pred for g in gold.lower()):\n",
    "#         correct += 1\n",
    "#     total += 1\n",
    "\n",
    "# print(f\"\\nAccuracy@10: {correct/total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0772b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nAccuracy@10: {correct/total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f758916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bf75ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Кэш моделей: C:\\Users\\Anastasiya Fedotova/.cache/huggingface/hub/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Стандартный путь кэша\n",
    "cache_path = os.path.expanduser('~/.cache/huggingface/hub/')\n",
    "print(f\"📁 Кэш моделей: {cache_path}\")\n",
    "\n",
    "# # Ищем конкретную модель\n",
    "# model_folder = f\"models--{model_name.replace('/', '--')}\"\n",
    "# full_path = os.path.join(cache_path, model_folder)\n",
    "# print(f\"📁 Папка модели: {full_path}\")\n",
    "\n",
    "# # Проверяем существует ли\n",
    "# if os.path.exists(full_path):\n",
    "#     print(\"✅ Модель найдена!\")\n",
    "#     # Покажем что внутри\n",
    "#     for item in os.listdir(full_path):\n",
    "#         print(f\"  📂 {item}\")\n",
    "# else:\n",
    "#     print(\"❌ Модель не найдена в кэше\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f847c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
