{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8a6c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, float16\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "DEVICE = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Current device is: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b89e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å ‚Ä¶\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e54ca57f6e74488b9050ed4b21f3d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anastasiya Fedotova\\Desktop\\DS&ML\\github\\rag_testing\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Anastasiya Fedotova\\.cache\\huggingface\\hub\\models--unsloth--Qwen2.5-3B-unsloth-bnb-4bit. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c356ad7ecef4230ab59f979ccd4e512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340bf07bb80e4c748ef5a89850c9c22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bcfcd83b5342d9ad489f5ccf5320ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03065165b2ca402dacd07d6a283c4934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c5e2fab20b421f846d5a4159f40b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a2b304eaa84c80ab7a55c1509e59e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec30974c8344d23b563d19c2517f092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5f75eb413e44349b2559ae00aaf733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# MODEL_ID = \"deepseek-ai/deepseek-coder-7b-instruct\"\n",
    "MODEL_ID = \"unsloth/Qwen2.5-3B-unsloth-bnb-4bit\"\n",
    "\n",
    "# –ö–≤–∞–Ω—Ç—É–µ–º –≤¬†4¬†–±–∏—Ç–∞, —á—Ç–æ–±—ã –ø–æ–º–µ—Å—Ç–∏–ª–æ—Å—å –≤¬†VRAM 6‚Äì8¬†–ì–ë\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                          # –≤–∫–ª—é—á–∏—Ç—å 4-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ\n",
    "    bnb_4bit_quant_type=\"nf4\",                  # —Ç–∏–ø –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è: \"nf4\" (Normalized Float 4) –∏–ª–∏ \"fp4\"\n",
    "    bnb_4bit_use_double_quant=True,             # –≤–∫–ª—é—á–∏—Ç—å –¥–≤–æ–π–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è)\n",
    "    bnb_4bit_compute_dtype=float16        # —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, bfloat16 (–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ T4), float16)\n",
    ")\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å ‚Ä¶\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ff9ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—ä—è—Å–Ω–∏, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —á—Ç–æ —Ç–∞–∫–æ–µ ‚ÄòState of the Union‚Äô. –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º –∞–±–∑–∞—Ü–µ–º –Ω–∞ —Ä—É—Å—Å–∫–æ–º. –í –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ —ç—Ç–æ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –æ–∑–Ω–∞—á–∞–µ—Ç ¬´–°–æ—Ü–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å¬ª, ¬´–°–æ—Å—Ç–æ—è–Ω–∏–µ –∑–¥–æ—Ä–æ–≤—å—è¬ª, ¬´–°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—â–µ—Å—Ç–≤–∞¬ª. –¢.–µ. –æ–Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –≤ —à–∏—Ä–æ–∫–æ–º —Å–º—ã—Å–ª–µ: –æ–± –æ–±—â–µ–º ¬´—É—Ä–æ–≤–Ω–µ¬ª –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è, —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω–∏, –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è —Å—Ç—Ä–∞–Ω—ã. –¢–∞–∫–∂–µ –æ–Ω–æ –º–æ–∂–µ—Ç —É–ø–æ—Ç—Ä–µ–±–ª—è—Ç—å—Å—è –≤ –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–º —Å–º—ã—Å–ª–µ, —á—Ç–æ–±—ã –æ—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π ¬´—É—Ä–æ–≤–µ–Ω—å¬ª (—Å—Ç–µ–ø–µ–Ω—å) —Ä–∞–∑–≤–∏—Ç–∏—è –∫–∞–∫–æ–π-–ª–∏–±–æ ¬´—Å—Ñ–µ—Ä—ã¬ª –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∂–∏–∑–Ω–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, —ç–∫–æ–Ω–æ–º–∏–∫–∏ –∏–ª–∏ –º–µ–¥–∏—Ü–∏–Ω—ã. –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ ¬´State of the Union¬ª —É–ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç—Å—è –≤ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–º —Å–º—ã—Å–ª–µ –∏ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –°–®–ê –¥–∞–µ—Ç –¥–æ–∫–ª–∞–¥ –æ–± –æ–±—â–µ–º ¬´—É—Ä–æ–≤–Ω–µ¬ª —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è —Å—Ç—Ä–∞–Ω—ã.\n"
     ]
    }
   ],
   "source": [
    "def ask_DeepSeek(prompt: str):\n",
    "    resp = llm_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=200,     # –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º ‚â§ 200 –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        do_sample=True,         # —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –¥–æ–¥—É–º—ã–≤–∞—Ç—å\n",
    "        truncation=True,        # –æ–±—Ä—ã–≤–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã\n",
    "        top_k=20,               # —Ç–æ–ø-20 –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –≤—ã—Ö–æ–¥–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "        num_return_sequences=1, # 1 –≤–∞—Ä–∏–∞–Ω—Ç –æ—Ç–≤–µ—Ç–∞\n",
    "        eos_token_id=tokenizer.eos_token_id, # —á—Ç–æ —Å—á–∏—Ç–∞—Ç—å –∫–æ–Ω—Ü–æ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "    )[0][\"generated_text\"]\n",
    "    print(resp)\n",
    "\n",
    "ask_DeepSeek(\"–û–±—ä—è—Å–Ω–∏, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —á—Ç–æ —Ç–∞–∫–æ–µ ‚ÄòState of the Union‚Äô. –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º –∞–±–∑–∞—Ü–µ–º –Ω–∞ —Ä—É—Å—Å–∫–æ–º.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bf3758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–æ–∫—É–º–µ–Ω—Ç–æ–≤: 6284\n",
      "–ï–≤–∞ ¬†‚Äî –≤ –∞–≤—Ä–∞–∞–º–∏—á–µ—Å–∫–∏—Ö —Ä–µ–ª–∏–≥–∏—è—Ö¬†‚Äî –ø—Ä–∞–º–∞—Ç–µ—Ä—å –≤—Å–µ—Ö –ª—é–¥–µ–π, –ø–µ—Ä–≤–∞—è –∂–µ–Ω—â–∏–Ω–∞, –∂–µ–Ω–∞ –ê–¥–∞–º–∞, —Å–æ–∑–¥–∞–Ω–Ω–∞—è –∏–∑ –µ–≥–æ —Ä–µ–±—Ä–∞, –º–∞—Ç—å –ö–∞–∏–Ω–∞, –ê–≤–µ–ª—è –∏ –°–∏—Ñ–∞.–ë–∏–±–ª–µ–π—Å–∫–∏–π —Ä–∞—Å—Å–∫–∞–∑ –æ —Å–æ—Ç–≤–æ—Ä–µ–Ω–∏–∏ –ê–¥–∞–º–∞ –∏ –ï–≤—ã, –≥—Ä–µ—Ö–æ–ø–∞–¥–µ–Ω–∏–∏ –∏ –∏–∑–≥–Ω–∞–Ω–∏ ‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "wiki_ds = load_dataset(\"Den4ikAI/russian_cleared_wikipedia\", split=\"train\")\n",
    "\n",
    "# Document\n",
    "corpus_docs = [\n",
    "    Document(page_content=rec[\"sample\"])\n",
    "    for rec in wiki_ds\n",
    "]\n",
    "\n",
    "print(\"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤:\", len(corpus_docs))\n",
    "print(corpus_docs[0].page_content[:200], \"‚Ä¶\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9105ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import TextLoader  # –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –∏—Ö –≤ –æ–±—ä–µ–∫—Ç—ã Document –¥–ª—è LangChain.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã (chunks).\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–∞.\n",
    "from langchain.vectorstores import Chroma  # –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ Chroma: —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏ –∏—â–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline  # –∏—Å–ø–æ–ª—å–∑—É–µ—Ç HuggingFace Transformers pipeline –∫–∞–∫ LLM-–º–æ–¥—É–ª—å –≤ LangChain.\n",
    "from langchain.chains import RetrievalQA  # –≥–æ—Ç–æ–≤–∞—è —Ü–µ–ø–æ—á–∫–∞ ¬´–ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞¬ª (Retrieval-augmented QA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73e996d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ß–∞–Ω–∫–æ–≤: 67536\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512, chunk_overlap=50\n",
    ")\n",
    "\n",
    "docs = splitter.split_documents(corpus_docs)\n",
    "print(\"–ß–∞–Ω–∫–æ–≤:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a2817a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fc8c44d3724f23b3ae1005dd848f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anastasiya Fedotova\\Desktop\\DS&ML\\github\\rag_testing\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Anastasiya Fedotova\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05057acf8d1644a98726e2ee3b6e10b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a6e9357f314048bc003fb02ab13292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3c49504db34907a2a993530ef83b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f70878ac07047f6b88ca9d86aa1da9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4849e413a9784952b52e97ae53b91168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d12fc768524a8aa0f7f19dc46b1702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6862f6cb04409b93edbf8d21878b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23d84e33e474eacb3eec25b14eb0d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e40dca185b46af9fe1a3ca873ed410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "C:\\Users\\Anastasiya Fedotova\\AppData\\Local\\Temp\\ipykernel_26980\\3103602615.py:12: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist() # –≤ –Ω–∞—à–µ–º —Ä–∞–±–æ—á–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–æ–∑–¥–∞–ª–∞—Å—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è - –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    # model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_name=\"intfloat/multilingual-e5-base\", #Russian semantic search\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,               # –ª–∏–±–æ corpus_docs, –µ—Å–ª–∏ –±–µ–∑ —Å–ø–ª–∏—Ç—Ç–µ—Ä–∞\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"chroma_ragmini\"  # –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã\n",
    ")\n",
    "vectordb.persist() # –≤ –Ω–∞—à–µ–º —Ä–∞–±–æ—á–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–æ–∑–¥–∞–ª–∞—Å—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è - –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "818e667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_cfg = GenerationConfig.from_pretrained(MODEL_ID)\n",
    "gen_cfg.max_length     = 2048     # –¥–æ–ø—É—Å—Ç–∏–º –ª—é–±–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ 2048 —Ç–æ–∫–µ–Ω–æ–≤\n",
    "gen_cfg.max_new_tokens = 128      # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞\n",
    "# gen_cfg.do_sample = True\n",
    "# gen_cfg.temperature = 0.0\n",
    "# gen_cfg.top_p = 0.9\n",
    "\n",
    "model.generation_config = gen_cfg   # –ø—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –∫ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff8bda59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "simple_custom_prompt = PromptTemplate(\n",
    "    template=\"\"\"<think>\n",
    "Use the following context to answer the question. Be precise and short. Avoid repetitions. Do not explain the answer unless asked to do so.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "First think logically, then give me the answer.\n",
    "</think>\n",
    "<answer>\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "simple_custom_prompt = PromptTemplate(\n",
    "    template=\"\"\"<think>\n",
    "–°–Ω–∞—á–∞–ª–∞ –ø–æ–¥—É–º–∞–π, –ø–æ—Ç–æ–º –æ—Ç–≤–µ—Ç—å. –ò–∑–±–µ–≥–∞–π –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –∏ –±—É–¥—å –∫—Ä–∞—Ç–æ–∫, –µ—Å–ª–∏ –Ω–µ –ø–æ–ø—Ä–æ—Å—è—Ç –ø–æ—è—Å–Ω–µ–Ω–∏–π.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "</think>\n",
    "<answer>\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=llm_pipeline,          # —Ç–æ—Ç, —á—Ç–æ –º—ã —Å–æ–±—Ä–∞–ª–∏ –¥–ª—è DeepSeek\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 128,      # –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º 128 –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        \"temperature\": 0.1,         # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ\n",
    "        \"do_sample\": False          # —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç –±—ã–ª –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω\n",
    "    }\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": simple_custom_prompt},  # –≤–∞—à –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç\n",
    "    return_source_documents=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af137e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—Ç–≤–µ—Ç: <answer>–°–∫–∞–∑–∫–∞ –æ —Ä—ã–±–∫–µ –∏ —Ä—ã–±–∞–∫–µ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â—É—é –∏—Å—Ç–æ—Ä–∏—é: –û–¥–Ω–∞–∂–¥—ã —Ä—ã–±–∞–∫ –ª–æ–≤–∏–ª —Ä—ã–±—É –Ω–∞ —Ä–µ–∫–µ, –∏ —Ä—ã–±–∞ –ø–æ–ø–∞–ª–∞ –≤ –µ–≥–æ –ª–æ–≤—É—à–∫—É. –†—ã–±–∫–∞ –±—ã–ª–∞ –æ—á–µ–Ω—å –∫—Ä–∞—Å–∏–≤–∞—è –∏ —É–º–Ω–∞—è. –û–Ω–∞ –∑–Ω–∞–ª–∞, —á—Ç–æ —Ä—ã–±–∞–∫ —Å–æ–±–∏—Ä–∞–ª—Å—è —É–π—Ç–∏, –∏ —Ä–µ—à–∏–ª–∞ —Ä–∞–∑–≤–ª–µ—á—å –µ–≥–æ –∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å. –†—ã–±–∫–∞ –Ω–∞—á–∞–ª–∞ –ø–µ—Ç—å –∏ –ø–ª—è—Å–∞—Ç—å –¥–ª—è —Ä—ã–±–∞–∫–∞, –∏ —Ä—ã–±–∞–∫ –ø–æ—Å–º–µ—è–ª—Å—è –Ω–∞–¥ –Ω–µ–π –∏ —Ä–µ—à–∏–ª –∑–∞—Å—Ç–∞–≤–∏—Ç—å –µ–µ —Ä–∞–±–æ—Ç–∞—Ç—å. –û–Ω —Å–∫–∞–∑–∞–ª —Ä—ã–±–∫–µ, —á—Ç–æ –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ –±—É–¥–µ—Ç –ª–æ–≤–∏—Ç—å—Å—è, –æ–Ω –µ–µ –æ—Ç–ø—É—Å—Ç–∏—Ç. –†—ã–±–∫–∞ —Å–æ–≥–ª–∞—Å–∏–ª–∞—Å—å –Ω–∞ —ç—Ç–æ –∏ —Å—Ç–∞–ª–∞ —É–ø–æ—Ä–Ω–æ –ø—ã—Ç–∞—Ç—å—Å—è –ª–æ–≤–∏—Ç—å —Ä—ã–±—É. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —Ä—ã–±–∞–∫ –ø—Ä–∏—à–ª–æ—Å—å –æ—Ç–ø—É—Å—Ç–∏—Ç—å —Ä—ã–±–∫—É, –∏ –æ–Ω–∞ —É—à–ª–∞ –≤ –≤–æ–¥—É. –†—ã–±–∫–∞ –ø–æ—à–ª–∞ –Ω–∞ –±–µ—Ä–µ–≥ –∏ —Å–∫–∞–∑–∞–ª–∞ —Ä—ã–±–∞–∫—É, —á—Ç–æ –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã–ª–æ –µ–π –±—ã—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ –ø–æ–º–æ–≥–ª–∞ –µ–º—É –ø–æ—á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å —Å–≤–æ—é —Å–∏–ª—É –∏ —É–ø–æ—Ä—Å—Ç–≤–æ. –°–∫–∞–∑–∫–∞ –æ —Ä—ã–±–∫–µ –∏ —Ä—ã–±–∞–∫–µ —É—á–∏—Ç –Ω–∞—Å, —á—Ç–æ –∏–Ω–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –±—ã—Ç—å —É—Å–µ—Ä–¥–Ω—ã–º –∏ –Ω–µ –±—Ä–æ—Å–∞—Ç—å—Å—è –≤ –ø–µ—Ä–µ–¥–µ–ª–∫–∏, –∞ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ, —Ç–æ –º–æ–∂–Ω–æ –±—ã—Ç—å —É–º–Ω—ã–º –∏ —É–±–µ–¥–∏—Ç—å –¥—Ä—É–≥–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞, –∞ –Ω–µ –∑–∞—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–≥–æ —Ä–∞–±–æ—Ç–∞—Ç—å, —á—Ç–æ–±—ã –¥–æ–±—Ä–∞—Ç—å—Å—è –¥–æ –Ω–µ–≥–æ.\n"
     ]
    }
   ],
   "source": [
    "question = \"–†–∞—Å—Å–∫–∞–∂–∏ —Å–∫–∞–∑–∫—É –ø—Ä–æ —Ä—ã–±–∞–∫–∞ –∏ —Ä—ã–±–∫—É\"\n",
    "\n",
    "answer_str = qa_chain.run(question)\n",
    "marker = \"</think>\"\n",
    "print(\"–û—Ç–≤–µ—Ç:\", answer_str.split(marker)[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b59d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Did Lincoln sign the National Banking Act of 1863? First say yes or no, then justify shortly.\"\n",
    "\n",
    "# answer_str = qa_chain.run(question)\n",
    "# print(\"–û—Ç–≤–µ—Ç:\",  answer_str.split(marker)[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d7857ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—Ç–≤–µ—Ç: <answer>–Ø–±–ª–æ–∫–∏ –∞–Ω—Ç–æ–Ω–æ–≤—Å–∫–æ–≥–æ —Å–æ—Ä—Ç–∞ –∏ –±–µ–ª—ã–π –Ω–∞–ª–∏–≤ - —ç—Ç–æ –¥–≤–∞ —Ä–∞–∑–Ω—ã—Ö —Å–æ—Ä—Ç–∞ —è–±–ª–æ–∫. –Ø–±–ª–æ–∫–∏ –∞–Ω—Ç–æ–Ω–æ–≤—Å–∫–æ–≥–æ —Å–æ—Ä—Ç–∞ –∏–º–µ—é—Ç –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∏ —è—Ä–∫–æ-–∫—Ä–∞—Å–Ω—ã–π —Ü–≤–µ—Ç —Å –±–µ–ª—ã–º–∏ —Ç–æ—á–∫–∞–º–∏ –Ω–∞ –∫–æ–∂—É—Ä–µ. –û–Ω–∏ –±–æ–≥–∞—Ç—ã –≤–∏—Ç–∞–º–∏–Ω–æ–º –° –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω–∏–∑–∫—É—é –∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –∏–¥–µ–∞–ª—å–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –¥–ª—è —Ç–µ—Ö, –∫—Ç–æ —Å–ª–µ–¥–∏—Ç –∑–∞ —Å–≤–æ–∏–º –≤–µ—Å–æ–º –∏–ª–∏ –¥–∏–µ—Ç–æ–π. –ë–µ–ª—ã–π –Ω–∞–ª–∏–≤, –Ω–∞–ø—Ä–æ—Ç–∏–≤, –∏–º–µ–µ—Ç –±–æ–ª–µ–µ –º–∞–ª–µ–Ω—å–∫–∏–µ —Ä–∞–∑–º–µ—Ä—ã –∏ –∏–º–µ–µ—Ç –±–æ–ª–µ–µ –º—è–≥–∫–∏–π –º—è–∫–æ—Ç—å. –û–Ω —Ç–∞–∫–∂–µ –±–æ–≥–∞—Ç –≤–∏—Ç–∞–º–∏–Ω–æ–º –° –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∏–∑–∫—É—é –∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å, –Ω–æ –µ–≥–æ –∫–∏—Å–ª–æ—Ç–Ω–æ—Å—Ç—å –±–æ–ª–µ–µ –Ω–∞—Å—ã—â–µ–Ω–∞, —á–µ–º —É –∞–Ω—Ç–æ–Ω–æ–≤—Å–∫–æ–≥–æ —Å–æ—Ä—Ç–∞.</answer>\n",
      "\n",
      "Source documents:\n",
      "–≤ —Ç–µ—Å—Ç–µ, –ø—Ä–∏–≥–æ—Ç–æ–≤–ª—è—é—Ç –Ω–∞—á–∏–Ω–∫–∏ –¥–ª—è –ø–∏—Ä–æ–≥–æ–≤, —Ç–æ—Ä—Ç–æ–≤ –∏ –ø–∏—Ä–æ–∂–Ω—ã—Ö, –æ—á–µ–Ω—å –ø–æ–ø—É–ª—è—Ä–Ω—ã —è–± ‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 1}), # –∑–¥–µ—Å—å –º—ã –ø–æ–¥–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª-–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –¥–æ–∫-–æ–≤\n",
    "    chain_type_kwargs={\"prompt\": simple_custom_prompt},\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# –ø–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é —Ñ—É–Ω–∫—Ü–∏—é - –æ–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n",
    "response = qa_chain.invoke(\"–ö–æ—Ä–æ—Ç–∫–æ —Ä–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ —Ä–∞–∑–Ω–∏—Ü—É —è–±–ª–æ–∫ –∞–Ω—Ç–æ–Ω–æ–≤–∫–∞ –∏ —è–±–ª–æ–∫ —Å–æ—Ä—Ç–∞ –±–µ–ª—ã–π –Ω–∞–ª–∏–≤\")\n",
    "print(\"–û—Ç–≤–µ—Ç:\", response[\"result\"].split(marker, 1)[1].strip())\n",
    "print(\"\\nSource documents:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(doc.page_content[:80], \"‚Ä¶\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cc7fc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='–≤ —Ç–µ—Å—Ç–µ, –ø—Ä–∏–≥–æ—Ç–æ–≤–ª—è—é—Ç –Ω–∞—á–∏–Ω–∫–∏ –¥–ª—è –ø–∏—Ä–æ–≥–æ–≤, —Ç–æ—Ä—Ç–æ–≤ –∏ –ø–∏—Ä–æ–∂–Ω—ã—Ö, –æ—á–µ–Ω—å –ø–æ–ø—É–ª—è—Ä–Ω—ã —è–±–ª–æ—á–Ω—ã–µ –ø–∏—Ä–æ–≥–∏.–°—É—à—ë–Ω—ã–µ —è–±–ª–æ–∫–∏ —è–≤–ª—è—é—Ç—Å—è —Ö–æ—Ä–æ—à–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –ª–µ–≥–∫–æ—É—Å–≤–∞–∏–≤–∞–µ–º—ã—Ö —Å–∞—Ö–∞—Ä–æ–≤, –º–∏–∫—Ä–æ—ç–ª–µ–º–µ–Ω—Ç–æ–≤, –∞ –≤ —Å–µ–º–µ–Ω–∞—Ö –æ–¥–Ω–æ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–ª–æ–¥–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –æ –æ —Å—É—Ç–æ—á–Ω–æ–π –Ω–æ—Ä–º—ã –π–æ–¥–∞.–ù–∞–ø—Ä–∏–º–µ—Ä, —è–±–ª–æ–∫–∏ –∞–Ω—Ç–æ–Ω–æ–≤—Å–∫–æ–≥–æ —Å–æ—Ä—Ç–∞ –≤ 100 –≥—Ä–∞–º–º–∞—Ö –ø—Ä–∏ –∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç–∏ –≤ 48 –∫–∫–∞–ª —Å–æ–¥–µ—Ä–∂–∞—Ç: 0,3 –≥ –±–µ–ª–∫–æ–≤, 11,5 –≥ —É–≥–ª–µ–≤–æ–¥–æ–≤, 0,02\\xa0–º–≥ –≤–∏—Ç–∞–º–∏–Ω–∞ B1, 4,9\\xa0–º–≥ –≤–∏—Ç–∞–º–∏–Ω–∞ –°, 16 –º–≥ –∫–∞–ª—å—Ü–∏—è –∏ 86 –º–≥ –∫–∞–ª–∏—è.–í–∏–¥—ã:–ï—â—ë –±–æ–ª–µ–µ 200 –≤–∏–¥–æ–≤—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π —ç—Ç–æ–≥–æ —Ä–æ–¥–∞ –∏–º–µ—é—Ç –≤ The Plant List —Å—Ç–∞—Ç—É—Å')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed2fa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json, tqdm\n",
    "\n",
    "# # —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –≤—Å–µ–≥–æ –¥–æ –∏ –≤–∫–ª—é—á–∞—è –º–∞—Ä–∫–µ—Ä </think>\n",
    "# def strip_cot(raw: str, marker: str = \"</think>\") -> str:\n",
    "#     parts = raw.split(marker, 1)\n",
    "#     # –µ—Å–ª–∏ –Ω–µ –≤–ª–µ–∑ COT –≤ —Ç–æ–∫–µ–Ω—ã, –≤—ã–≤–æ–¥–∏–º –æ—Ç–≤–µ—Ç –∫–∞–∫ –µ—Å—Ç—å\n",
    "#     return parts[1].strip() if len(parts) == 2 else raw.strip()\n",
    "\n",
    "# testset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\")[\"test\"]\n",
    "\n",
    "# total, correct = 0, 0\n",
    "\n",
    "# print(\"–ü—Ä–æ–≤–µ—Ä–∫–∞ 10 –ø—Ä–∏–º–µ—Ä–æ–≤:\\n\")\n",
    "# for sample in tqdm.tqdm(testset.select(range(4, 14))):\n",
    "#     q, gold = sample[\"question\"], sample[\"answer\"]\n",
    "\n",
    "#     result_dict = qa_chain.invoke(q)\n",
    "#     raw = result_dict[\"result\"]\n",
    "#     # –æ–±—Ä–µ–∑–∞–µ–º COT\n",
    "#     pred = strip_cot(raw)\n",
    "\n",
    "#     print(f\"\\n–í–æ–ø—Ä–æ—Å: {q}\")\n",
    "#     print(f\"–û–∂–∏–¥–∞–µ–º—ã–µ –æ—Ç–≤–µ—Ç—ã: {gold}\")\n",
    "#     print(f\"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {pred}\")\n",
    "\n",
    "#     if any(g.lower() in pred for g in gold.lower()):\n",
    "#         correct += 1\n",
    "#     total += 1\n",
    "\n",
    "# print(f\"\\nAccuracy@10: {correct/total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0772b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nAccuracy@10: {correct/total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f758916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bf75ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ –ö—ç—à –º–æ–¥–µ–ª–µ–π: C:\\Users\\Anastasiya Fedotova/.cache/huggingface/hub/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—É—Ç—å –∫—ç—à–∞\n",
    "cache_path = os.path.expanduser('~/.cache/huggingface/hub/')\n",
    "print(f\"üìÅ –ö—ç—à –º–æ–¥–µ–ª–µ–π: {cache_path}\")\n",
    "\n",
    "# # –ò—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "# model_folder = f\"models--{model_name.replace('/', '--')}\"\n",
    "# full_path = os.path.join(cache_path, model_folder)\n",
    "# print(f\"üìÅ –ü–∞–ø–∫–∞ –º–æ–¥–µ–ª–∏: {full_path}\")\n",
    "\n",
    "# # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏\n",
    "# if os.path.exists(full_path):\n",
    "#     print(\"‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞!\")\n",
    "#     # –ü–æ–∫–∞–∂–µ–º —á—Ç–æ –≤–Ω—É—Ç—Ä–∏\n",
    "#     for item in os.listdir(full_path):\n",
    "#         print(f\"  üìÇ {item}\")\n",
    "# else:\n",
    "#     print(\"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f847c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
